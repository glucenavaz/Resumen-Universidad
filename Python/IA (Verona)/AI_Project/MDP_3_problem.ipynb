{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Planning-Lab Lesson 3: Markov Decision Process\n",
    "\n",
    "In the third session we will work on the Markov decision process (MDP)\n",
    "\n",
    "## Lava environments\n",
    "The environments used are LavaFloor (visible in the figure) and its variations.\n",
    "\n",
    "![Lava](images/lava.png)\n",
    "\n",
    "The agent starts in cell $(0, 0)$ and has to reach the treasure in $(2, 3)$. In addition to the walls of the previous environments, the floor is covered with lava, there is a black pit of death.\n",
    "\n",
    "Moreover, the agent can't comfortably perform its actions that instead have a stochastic outcome (visible in the figure):\n",
    "\n",
    "![Dynact](images/dynact.png)\n",
    "\n",
    "The action dynamics is the following:\n",
    "- $P(0.8)$ of moving **in the desired direction**\n",
    "- $P(0.1)$ of moving in a direction 90° with respect to the desired direction\n",
    "\n",
    "Finally, since the floor is covered in lava, the agent receives a negative reward for each of its steps!\n",
    "\n",
    "- -0.04 for each lava cell (L)\n",
    "- -5 for the black pit (P). End of episode\n",
    "- +1 for the treasure (G). End of episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, random\n",
    "module_path = os.path.abspath(os.path.join('../tools'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import gym, envs\n",
    "from utils.ai_lab_functions import *\n",
    "from timeit import default_timer as timer\n",
    "from tqdm import tqdm as tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Properties \n",
    "\n",
    "In addition to the varables of the environments you have been using in the previous sessions, there are also a few more:\n",
    "\n",
    "- $T$: matrix of the transition function $T(s, a, s') \\rightarrow [0, 1]$\n",
    "- $RS$: matrix of the reward function $R(s) \\rightarrow \\mathbb{R}$\n",
    "\n",
    "The available actions are still Left, Right, Up, Down.\n",
    "\n",
    "#### Code Hints:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of actions:  4\n",
      "Actions:  {0: 'L', 1: 'R', 2: 'U', 3: 'D'}\n",
      "Reward of starting state: -0.04\n",
      "Reward of goal state: 1.0\n",
      "Probability from (0, 0) to (0, 1) with action right: 0.8\n",
      "Probability from (0, 0) to (2, 3) with action right: 0.0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"LavaFloor-v0\")\n",
    "\n",
    "current_state = env.pos_to_state(0, 0)\n",
    "next_state = env.pos_to_state(0, 1)\n",
    "goal_state = env.pos_to_state(2, 3)\n",
    "\n",
    "print(\"Number of actions: \", env.action_space.n)\n",
    "print(\"Actions: \", env.actions)\n",
    "print(\"Reward of starting state:\", env.RS[current_state])\n",
    "print(\"Reward of goal state:\", env.RS[goal_state])\n",
    "print(\"Probability from (0, 0) to (0, 1) with action right:\", env.T[current_state, 1, next_state])\n",
    "print(\"Probability from (0, 0) to (2, 3) with action right:\", env.T[current_state, 1, goal_state])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percentage of time agent reaches the state to the right:  76.0\n",
      "Transition model for  LavaFloor-v0  : \n",
      "probability of reaching  (0, 1) from  (0, 0)  executing action  L  :  0.0\n",
      "probability of reaching  (0, 1) from  (0, 0)  executing action  R  :  0.8\n",
      "probability of reaching  (0, 1) from  (0, 0)  executing action  U  :  0.1\n",
      "probability of reaching  (0, 1) from  (0, 0)  executing action  D  :  0.1\n",
      "Reward for non terminal states:  -0.04\n",
      "Reward for state : (1, 3)  (state type:  P ) :  -5.0\n",
      "Reward for state : (2, 3)  (state type:  G ) :  1.0\n"
     ]
    }
   ],
   "source": [
    "env_name = \"LavaFloor-v0\"\n",
    "env = gym.make(env_name)\n",
    "\n",
    "c=0\n",
    "state_right = env.pos_to_state(0, 1) #state to the tight of start state\n",
    "for i in range(1,101):\n",
    "    current_state = env.pos_to_state(0, 0)\n",
    "    state = env.sample(current_state, 1) #trying to go right\n",
    "    if (state==state_right): \n",
    "        c+=1 #counting how many times the agent reaches the state to the right\n",
    "        \n",
    "#computing percentage of time agent reached the state to the right going right, should be around 80%           \n",
    "print(\"percentage of time agent reaches the state to the right: \", c/i*100) \n",
    "\n",
    "print(\"Transition model for \", env_name, \" : \") #assume transition functions is the same for all states\n",
    "state=0\n",
    "next_state=1\n",
    "for i in range(0,env.action_space.n):\n",
    "    print(\"probability of reaching \", env.state_to_pos(next_state), \"from \", env.state_to_pos(state), \" executing action \", env.actions[i], \" : \", env.T[state, i, next_state])\n",
    "print(\"Reward for non terminal states: \",env.RS[env.pos_to_state(0,0)]) #assume all states have same reward\n",
    "for state in range(0,env.observation_space.n):\n",
    "    if env.grid[state] == \"P\" or env.grid[state] == \"G\":\n",
    "                    print(\"Reward for state :\", env.state_to_pos(state) ,\" (state type: \", env.grid[state],\") : \",env.RS[state])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 1: Value Iteration Algorithm\n",
    "\n",
    "Your first assignment is to implement the Value Iteration algorithm on LavaFloor. The solution returned by your algorithm must be a 1-d array of action identifiers where the $i$-th action refers to the $i$-th state.  You can perform all the test on a different versions of the environment, but with the same structure: *HugeLavaFloor*, *NiceLavaFloor* and *VeryBadLavaFloor*.\n",
    "\n",
    "<img src=\"images/value-iteration.png\" width=\"600\">\n",
    "\n",
    "The *value_iteration* function has to be implemented. Notice that the value iteration approach return a matrix with the value for eacht state, the function *values_to_policy* automatically convert this matrix in the policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(environment, maxiters=300, discount=0.5, max_error=1e-3):\n",
    "    \"\"\"\n",
    "    Performs the value iteration algorithm for a specific environment\n",
    "    \n",
    "    Args:\n",
    "        environment: OpenAI Gym environment\n",
    "        maxiters: timeout for the iterations\n",
    "        discount: gamma value, the discount factor for the Bellman equation\n",
    "        max_error: the maximum error allowd in the utility of any state\n",
    "        \n",
    "    Returns:\n",
    "        policy: 1-d dimensional array of action identifiers where index `i` corresponds to state id `i`\n",
    "    \"\"\"\n",
    "    \n",
    "    U_1 = [0 for _ in range(environment.observation_space.n)] # vector of utilities for states S\n",
    "    delta = 0 # maximum change in the utility o any state in an iteration\n",
    "    U = U_1.copy()\n",
    "    #\n",
    "    # Code Here!\n",
    "    #\n",
    "    for it in range(maxiters):\n",
    "        delta = 0\n",
    "        for state in range(environment.observation_space.n):\n",
    "            if env.grid[state] == \"P\" or env.grid[state] == \"G\":\n",
    "                U_1[state] = environment.RS[state]  # Directly assign reward to terminal states\n",
    "                continue\n",
    "            #Bellman: compute maximum of all the actions\n",
    "            max_u = -9999999\n",
    "            best_action = None\n",
    "            for action in range(environment.action_space.n):\n",
    "                u = 0\n",
    "                for next_state in range(environment.observation_space.n):\n",
    "                    prob = environment.T[state, action, next_state]    #probability of transition\n",
    "                    reward = environment.RS[state]   #reward actual state\n",
    "                    u += prob * (reward + discount * U[next_state])\n",
    "                max_u = max(max_u, u)\n",
    "            U_1[state] = max_u     #asigna a U'[state] el máximo de la fórmula Bellman (en u)\n",
    "            delta = max(delta, abs(U_1[state] - U[state]))\n",
    "    \n",
    "        U = U_1.copy()    #actualizamos  U\n",
    "    \n",
    "        if delta < max_error * (1-discount)/discount:    #Convergencia\n",
    "            break\n",
    "    \n",
    "    return values_to_policy(np.asarray(U), env) # automatically convert the value matrix U to a policy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The following code executes Value Iteration and prints the resulting policy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env_name = \"LavaFloor-v0\"\n",
    "env_name = \"HugeLavaFloor-v0\"\n",
    "#env_name = \"NiceLavaFloor-v0\"\n",
    "#env_name = \"VeryBadLavaFloor-v0\"\n",
    "\n",
    "\n",
    "\n",
    "env = gym.make(env_name)\n",
    "print(\"\\nENV RENDER:\")\n",
    "env.render()\n",
    "\n",
    "print(\"Transition model for \", env_name, \" : \") #assume transition functions is the same for all states\n",
    "state=0\n",
    "next_state=1\n",
    "for i in range(0,env.action_space.n):\n",
    "    print(\"probability of reaching \", env.state_to_pos(next_state), \"from \", env.state_to_pos(state), \" executing action \", env.actions[i], \" : \", env.T[state, i, next_state])\n",
    "print(\"Reward for non terminal states: \",env.RS[env.pos_to_state(0,0)]) #assume all states have same reward\n",
    "for state in range(0,env.observation_space.n):\n",
    "    if env.grid[state] == \"P\" or env.grid[state] == \"G\":\n",
    "                    print(\"Reward for state :\", env.state_to_pos(state) ,\" (state type: \", env.grid[state],\") : \",env.RS[state])\n",
    "\n",
    "t = timer()\n",
    "policy = value_iteration(env)\n",
    "\n",
    "print(\"\\nEXECUTION TIME: \\n{}\".format(round(timer() - t, 4)))\n",
    "policy_render = np.vectorize(env.actions.get)(policy.reshape(env.rows, env.cols))\n",
    "results = CheckResult_L3(env_name, policy_render)\n",
    "results.check_value_iteration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(environment, maxiters=300, discount=0.1, max_error=1e-3):\n",
    "    \"\"\"\n",
    "    Performs the value iteration algorithm for a specific environment\n",
    "    \n",
    "    Args:\n",
    "        environment: OpenAI Gym environment\n",
    "        maxiters: timeout for the iterations\n",
    "        discount: gamma value, the discount factor for the Bellman equation\n",
    "        max_error: the maximum error allowd in the utility of any state\n",
    "        \n",
    "    Returns:\n",
    "        policy: 1-d dimensional array of action identifiers where index `i` corresponds to state id `i`\n",
    "        utilities: 1-d dimensional array of utility values for each state\n",
    "    \"\"\"\n",
    "    \n",
    "    U_1 = [0 for _ in range(environment.observation_space.n)]  # Vector of utilities for states S\n",
    "    delta = 0  # Maximum change in the utility of any state in an iteration\n",
    "    U = U_1.copy()\n",
    "    \n",
    "    for it in range(maxiters):\n",
    "        delta = 0\n",
    "        for state in range(environment.observation_space.n):\n",
    "            if environment.grid[state] in [\"P\", \"G\"]:  # Handle terminal states\n",
    "                U_1[state] = environment.RS[state]  # Assign reward directly\n",
    "                continue\n",
    "            \n",
    "            # Bellman: Compute maximum utility for all possible actions\n",
    "            max_u = -99999999\n",
    "            for action in range(environment.action_space.n):\n",
    "                u = 0\n",
    "                for next_state in range(environment.observation_space.n):\n",
    "                    prob = environment.T[state, action, next_state]  # Transition probability\n",
    "                    reward = environment.RS[state]  # Reward of the current state\n",
    "                    u += prob * (reward + discount * U[next_state])\n",
    "                max_u = max(max_u, u)\n",
    "            U_1[state] = max_u  # Assign max Bellman value to U'\n",
    "            delta = max(delta, abs(U_1[state] - U[state]))\n",
    "        \n",
    "        U = U_1.copy()  # Update U\n",
    "        \n",
    "        if delta < max_error * (1 - discount) / discount:  # Convergence check\n",
    "            break\n",
    "    \n",
    "    policy = values_to_policy(np.asarray(U), environment)  # Convert utilities to policy\n",
    "    return policy, U  # Return both policy and utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utility of state (2,2): 0.03995200000000001\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Configuración del entorno (reemplaza con la configuración real si es necesario)\n",
    "env_name = \"LavaFloor-v0\"  # Define correctamente tu entorno\n",
    "environment = gym.make(env_name)\n",
    "\n",
    "# Ejecutar el algoritmo de Value Iteration\n",
    "policy, utilities = value_iteration(environment, discount=0.1)\n",
    "\n",
    "# Determinar la utilidad del estado (x,y)\n",
    "state = environment.pos_to_state(2,2)  # Convertir coordenadas a índice de estado\n",
    "utility = utilities[state]\n",
    "\n",
    "print(f\"Utility of state (2,2): {utility}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 2: Policy Iteration Algorithm (<span style=\"color:red\">*OPTIONAL*</span>)\n",
    "\n",
    "Your <span style=\"color:red\"> *optional*</span> assignment is to implement the Policy Iteration algorithm on LavaFloor. The solution returned by your algorithm must be a 1-d array of action identifiers where the $i$-th action refers to the $i$-th state. You can perform all the test on a different versions of the environment, but with the same structure: *HugeLavaFloor*, *NiceLavaFloor* and *VeryBadLavaFloor*.\n",
    "\n",
    "<img src=\"images/policy-iteration.png\" width=\"600\">\n",
    "\n",
    "For the *policy evaluation step*, it is necessary to implement this function:\n",
    "\n",
    "<img src=\"images/policy-evaluating.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The following function has to be implemented:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(environment, maxiters=150, discount=0.9, maxviter=10):\n",
    "    \"\"\"\n",
    "    Performs the policy iteration algorithm for a specific environment\n",
    "    \n",
    "    Args:\n",
    "        environment: OpenAI Gym environment\n",
    "        maxiters: timeout for the iterations\n",
    "        discount: gamma value, the discount factor for the Bellman equation\n",
    "        \n",
    "    Returns:\n",
    "        policy: 1-d dimensional array of action identifiers where index `i` corresponds to state id `i`\n",
    "    \"\"\"\n",
    "    \n",
    "    policy = [0 for _ in range(environment.observation_space.n)] #initial policy\n",
    "    U = [0 for _ in range(environment.observation_space.n)] #utility array\n",
    "\n",
    "    # Step (1): Policy Evaluation\n",
    "    #\n",
    "    # Code Here!\n",
    "    #\n",
    "    \n",
    "    # Step (2) Policy Improvement\n",
    "    unchanged = True  \n",
    "    #\n",
    "    # Code Here!\n",
    "    #\n",
    "    \n",
    "    return np.asarray(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The following code executes and Value Iteration and prints the resulting policy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ENV RENDER:\n",
      "[['S' 'L' 'L' 'L']\n",
      " ['L' 'W' 'L' 'P']\n",
      " ['L' 'L' 'L' 'G']]\n",
      "Transition model for  LavaFloor-v0  : \n",
      "probability of reaching  (0, 1) from  (0, 0)  executing action  L  :  0.0\n",
      "probability of reaching  (0, 1) from  (0, 0)  executing action  R  :  0.8\n",
      "probability of reaching  (0, 1) from  (0, 0)  executing action  U  :  0.1\n",
      "probability of reaching  (0, 1) from  (0, 0)  executing action  D  :  0.1\n",
      "Reward for non terminal states:  -0.04\n",
      "Reward for state : (1, 3)  (state type:  P ) :  -5.0\n",
      "Reward for state : (2, 3)  (state type:  G ) :  1.0\n",
      "\n",
      "EXECUTION TIME: \n",
      "0.0001\n",
      "\n",
      "\u001b[96m##################################################################\u001b[0m\n",
      "\u001b[96m#######  Environment: LavaFloor-v0 \tPolicy Iteration  ########\u001b[0m\n",
      "\u001b[96m##################################################################\u001b[0m\n",
      "\n",
      "\u001b[91m> Your policy\n",
      " [['L' 'L' 'L' 'L']\n",
      " ['L' 'L' 'L' 'L']\n",
      " ['L' 'L' 'L' 'L']] is not optimal!\n",
      "\n",
      "Our policy is:\n",
      " [['D' 'L' 'L' 'U']\n",
      " ['D' 'L' 'L' 'L']\n",
      " ['R' 'R' 'R' 'L']]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env_name = \"LavaFloor-v0\"\n",
    "#env_name = \"HugeLavaFloor-v0\"\n",
    "#env_name = \"NiceLavaFloor-v0\"\n",
    "#env_name = \"VeryBadLavaFloor-v0\"\n",
    "\n",
    "env = gym.make(env_name)\n",
    "print(\"\\nENV RENDER:\")\n",
    "env.render()\n",
    "\n",
    "print(\"Transition model for \", env_name, \" : \") #assume transition functions is the same for all states\n",
    "state=0\n",
    "next_state=1\n",
    "for i in range(0,env.action_space.n):\n",
    "    print(\"probability of reaching \", env.state_to_pos(next_state), \"from \", env.state_to_pos(state), \" executing action \", env.actions[i], \" : \", env.T[state, i, next_state])\n",
    "print(\"Reward for non terminal states: \",env.RS[env.pos_to_state(0,0)]) #assume all states have same reward\n",
    "for state in range(0,env.observation_space.n):\n",
    "    if env.grid[state] == \"P\" or env.grid[state] == \"G\":\n",
    "                    print(\"Reward for state :\", env.state_to_pos(state) ,\" (state type: \", env.grid[state],\") : \",env.RS[state])\n",
    "\n",
    "t = timer()\n",
    "policy = policy_iteration(env)\n",
    "\n",
    "print(\"\\nEXECUTION TIME: \\n{}\".format(round(timer() - t, 4)))\n",
    "policy_render = np.vectorize(env.actions.get)(policy.reshape(env.rows, env.cols))\n",
    "results = CheckResult_L3(env_name, policy_render)\n",
    "results.check_policy_iteration()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison\n",
    "\n",
    "The following code performs a comparison between Value Iteration and Policy Iteration, by plotting the accumulated rewards of each episode with iterations in range $[1, 50]$ (might take a long time if not optimizied via numpy). You can perform all the test on a different versions of the environment, but with the same structure: *HugeLavaFloor*.\n",
    "\n",
    "The function **run_episode(envirnonment, policy, max_iteration)** run an episode on the given environment using the input policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transition model for  LavaFloor-v0  : \n",
      "probability of reaching  (0, 1) from  (0, 0)  executing action  L  :  0.0\n",
      "probability of reaching  (0, 1) from  (0, 0)  executing action  R  :  0.8\n",
      "probability of reaching  (0, 1) from  (0, 0)  executing action  U  :  0.1\n",
      "probability of reaching  (0, 1) from  (0, 0)  executing action  D  :  0.1\n",
      "Reward for non terminal states:  -0.04\n",
      "Reward for state : (1, 3)  (state type:  P ) :  -5.0\n",
      "Reward for state : (2, 3)  (state type:  G ) :  1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Value Iteration:   0%|                                                                          | 0/50 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "'p' must be 1-dimensional",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 37\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# Repeat multiple times and compute mean reward\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(rep):\n\u001b[1;32m---> 37\u001b[0m     reprew \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m run_episode(env, policy, elimit)  \u001b[38;5;66;03m# Execute policy\u001b[39;00m\n\u001b[0;32m     38\u001b[0m virewards[c] \u001b[38;5;241m=\u001b[39m reprew \u001b[38;5;241m/\u001b[39m rep\n\u001b[0;32m     39\u001b[0m c \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32m~\\AI_Lab\\tools\\utils\\ai_lab_functions.py:143\u001b[0m, in \u001b[0;36mrun_episode\u001b[1;34m(environment, policy, limit)\u001b[0m\n\u001b[0;32m    141\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done \u001b[38;5;129;01mand\u001b[39;00m s \u001b[38;5;241m<\u001b[39m limit:\n\u001b[1;32m--> 143\u001b[0m     obs, r, done, _ \u001b[38;5;241m=\u001b[39m environment\u001b[38;5;241m.\u001b[39mstep(policy[obs])\n\u001b[0;32m    144\u001b[0m     reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m r\n\u001b[0;32m    145\u001b[0m     s \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32m~\\AI_Lab\\tools\\envs\\obsgrid_env.py:120\u001b[0m, in \u001b[0;36mObsGrid.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone:\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 120\u001b[0m ns \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msample(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrstate, action)\n\u001b[0;32m    121\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mR[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrstate, action, ns]\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrstate \u001b[38;5;241m=\u001b[39m ns\n",
      "File \u001b[1;32m~\\AI_Lab\\tools\\envs\\obsgrid_env.py:164\u001b[0m, in \u001b[0;36mObsGrid.sample\u001b[1;34m(self, state, action)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msample\u001b[39m(\u001b[38;5;28mself\u001b[39m, state, action):\n\u001b[0;32m    154\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;124;03m    Returns a new state sampled from the ones that can be reached from ``state`` executing ``action``\u001b[39;00m\n\u001b[0;32m    156\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;124;03m        reached state\u001b[39;00m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnp_random\u001b[38;5;241m.\u001b[39mchoice(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstaterange, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mT[state, action])\n",
      "File \u001b[1;32mnumpy\\\\random\\\\mtrand.pyx:966\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.RandomState.choice\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: 'p' must be 1-dimensional"
     ]
    }
   ],
   "source": [
    "env_name = \"LavaFloor-v0\"\n",
    "#env_name = \"HugeLavaFloor-v0\"\n",
    "\n",
    "print(\"Transition model for \", env_name, \" : \") #assume transition functions is the same for all states\n",
    "state=0\n",
    "next_state=1\n",
    "for i in range(0,env.action_space.n):\n",
    "    print(\"probability of reaching \", env.state_to_pos(next_state), \"from \", env.state_to_pos(state), \" executing action \", env.actions[i], \" : \", env.T[state, i, next_state])\n",
    "print(\"Reward for non terminal states: \",env.RS[env.pos_to_state(0,0)]) #assume all states have same reward\n",
    "for state in range(0,env.observation_space.n):\n",
    "    if env.grid[state] == \"P\" or env.grid[state] == \"G\":\n",
    "                    print(\"Reward for state :\", env.state_to_pos(state) ,\" (state type: \", env.grid[state],\") : \",env.RS[state])\n",
    "\n",
    "\n",
    "\n",
    "maxiters = 49\n",
    "\n",
    "env = gym.make(env_name)\n",
    "\n",
    "series = []  # Series of learning rates to plot\n",
    "liters = np.arange(maxiters + 1)  # Learning iteration values\n",
    "liters[0] = 1\n",
    "elimit = 100  # Limit of steps per episode\n",
    "rep = 10  # Number of repetitions per iteration value\n",
    "virewards = np.zeros(len(liters))  # Rewards array\n",
    "c = 0\n",
    "\n",
    "t = timer()\n",
    "\n",
    "# Value iteration\n",
    "for i in tqdm(liters, desc=\"Value Iteration\", leave=True):\n",
    "    reprew = 0\n",
    "    policy = value_iteration(env, maxiters=i)  # Compute policy\n",
    "        \n",
    "    # Repeat multiple times and compute mean reward\n",
    "    for _ in range(rep):\n",
    "        reprew += run_episode(env, policy, elimit)  # Execute policy\n",
    "    virewards[c] = reprew / rep\n",
    "    c += 1\n",
    "series.append({\"x\": liters, \"y\": virewards, \"ls\": \"-\", \"label\": \"Value Iteration\"})\n",
    "\n",
    "\n",
    "vmaxiters = 5  # Max number of iterations to perform while evaluating a policy\n",
    "pirewards = np.zeros(len(liters))  # Rewards array\n",
    "c = 0\n",
    "\n",
    "# Policy iteration\n",
    "for i in tqdm(liters, desc=\"Policy Iteration\", leave=True):\n",
    "    reprew = 0\n",
    "    policy = policy_iteration(env, maxiters=i)  # Compute policy\n",
    "    # Repeat multiple times and compute mean reward\n",
    "    for _ in range(rep):\n",
    "        reprew += run_episode(env, policy, elimit)  # Execute policy\n",
    "    pirewards[c] = reprew / rep\n",
    "    c += 1\n",
    "series.append({\"x\": liters, \"y\": pirewards, \"ls\": \"-\", \"label\": \"Policy Iteration\"})\n",
    "\n",
    "print(\"Execution time: {0}s\".format(round(timer() - t, 4)))\n",
    "np.set_printoptions(linewidth=10000)\n",
    "\n",
    "plot(series, \"Learning Rate\", \"Iterations\", \"Reward\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correct results for comparison can be found here below. Notice that since the executions are stochastic the charts could differ: the important thing is the global trend and the final convergence to an optimal solution.\n",
    "\n",
    "**Standard Lava floor results comparison**\n",
    "<img src=\"images/results-standard.png\" width=\"600\">\n",
    "\n",
    "**Huge Lava floor results comparison** \n",
    "<img src=\"images/results-huge.png\" width=\"600\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ai-lab]",
   "language": "python",
   "name": "conda-env-ai-lab-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
